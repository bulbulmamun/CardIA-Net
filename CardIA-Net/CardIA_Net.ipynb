{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519fc03e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc422e78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Linux-4.18.0-553.58.1.el8_10.x86_64-x86_64-with-glibc2.28\n",
      "PyTorch Version: 2.7.1+cu126\n",
      "Using device: cuda\n",
      "Expecting weight file: CardIA_Net_Trained_parameters.pth\n",
      "Expecting test set   : PTB_CSN_test_set.h5\n",
      "Loaded test_signals shape: (79070, 4, 300)\n",
      "Loaded test_labels shape : (79070,)\n",
      "Inferred CH_No from test set: 4\n",
      "Test DataLoader ready.\n",
      "InceptionTimePlus(\n",
      "  (backbone): Sequential(\n",
      "    (0): InceptionBlockPlus(\n",
      "      (inception): ModuleList(\n",
      "        (0): InceptionModulePlus(\n",
      "          (bottleneck): ConvBlock(\n",
      "            (conv): Conv1d(4, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (convs): ModuleList(\n",
      "            (0): ConvBlock(\n",
      "              (conv): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (1): ConvBlock(\n",
      "              (conv): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (2): ConvBlock(\n",
      "              (conv): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (mp_conv): Sequential(\n",
      "            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "            (1): ConvBlock(\n",
      "              (conv): Conv1d(4, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (concat): Identity()\n",
      "          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "        (1-5): 5 x InceptionModulePlus(\n",
      "          (bottleneck): ConvBlock(\n",
      "            (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (convs): ModuleList(\n",
      "            (0): ConvBlock(\n",
      "              (conv): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (1): ConvBlock(\n",
      "              (conv): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (2): ConvBlock(\n",
      "              (conv): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (mp_conv): Sequential(\n",
      "            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "            (1): ConvBlock(\n",
      "              (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (concat): Identity()\n",
      "          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (shortcut): ModuleList(\n",
      "        (0): ConvBlock(\n",
      "          (conv): Conv1d(4, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ModuleList(\n",
      "        (0-1): 2 x ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): GAP1d(\n",
      "      (gap): AdaptiveAvgPool1d(output_size=1)\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "  )\n",
      "  (attention): Attention(\n",
      "    (attention_weights): Linear(in_features=128, out_features=128, bias=False)\n",
      "  )\n",
      "  (final): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (linear): Linear(in_features=128, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Loaded trained parameters from CardIA_Net_Trained_parameters.pth\n",
      "Testing Done.\n",
      "Test Loss: 0.0697\n",
      "Test Accuracy: 98.45%\n",
      "Confusion Matrix:\n",
      "[[35465     5   535]\n",
      " [    5 10670    17]\n",
      " [  646    15 31712]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     36005\n",
      "           1       1.00      1.00      1.00     10692\n",
      "           2       0.98      0.98      0.98     32373\n",
      "\n",
      "    accuracy                           0.98     79070\n",
      "   macro avg       0.99      0.99      0.99     79070\n",
      "weighted avg       0.98      0.98      0.98     79070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CardIA-Net: An Explainable Deep Learning Model for MI Detection with ECG Lead Optimization.\n",
    "\n",
    "This module provides a reusable PyTorch implementation of the CardIA-Net model:\n",
    "- Multi-scale 1D Inception modules with residual shortcut\n",
    "- Channel-preserving Inception block (stack of Inception modules + residual add)\n",
    "- Global average pooling head\n",
    "- Lightweight vector attention over the pooled features\n",
    "- Linear classifier\n",
    "\n",
    "Typical input shape: (batch_size, num_leads, signal_length)\n",
    "\n",
    "Preprint of this study: \n",
    "Bulbul, Abdullah Al-Mamun and Awal, Md Abdul and Aloteibi, Saad and Moni, Mohammad Ali, \n",
    "Cardia-Net: An Explainable Deep Learning Model for Mi Detection with Ecg Lead Optimization. \n",
    "Available at SSRN: https://ssrn.com/abstract=5387266 or http://dx.doi.org/10.2139/ssrn.5387266\n",
    "\n",
    "PyTorch >= 1.10\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "## CardIA-Net Model - TEST ONLY (load params + test set)\n",
    "#######################################################################################\n",
    "\n",
    "# Imports (kept minimal, no changes to architecture code)\n",
    "import sys\n",
    "import platform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Reproducibility \n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Device setup\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device_str = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Using device: {device_str}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "########### Match your naming exactly ###########\n",
    "index_combination = 31   \n",
    "mod_weight = f'CardIA_Net_Trained_parameters.pth'  # load from working directory\n",
    "PTB_CSN_test = f'PTB_CSN_test_set.h5'  # load from working directory\n",
    "print(\"Expecting weight file:\", mod_weight)\n",
    "print(\"Expecting test set   :\", PTB_CSN_test)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 1000      # Number of training epochs\n",
    "batch_size = 32        # Batch Size\n",
    "num_incep_block = 6    # Number of Inception Blocks\n",
    "Num_CH = 4             # Number of input ECG leads\n",
    "lr = 0.0001            # Learning rate\n",
    "sig_len = 300          # Number of data-points in each ECG segment @500Hz\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "# Load the test dataset from the directory\n",
    "#######################################################################################\n",
    "with h5py.File(PTB_CSN_test, 'r') as hf:\n",
    "    test_signals = hf['test_signals'][:]\n",
    "    test_labels = hf['test_labels'][:]\n",
    "\n",
    "# Ensure shapes align\n",
    "test_labels = test_labels.reshape(-1)\n",
    "print(\"Loaded test_signals shape:\", test_signals.shape)\n",
    "print(\"Loaded test_labels shape :\", test_labels.shape)\n",
    "\n",
    "# Determine channels from the saved test set\n",
    "CH_No = test_signals.shape[1]\n",
    "print(\"Inferred CH_No from test set:\", CH_No)\n",
    "\n",
    "# Dataloader (same helper behavior)\n",
    "def create_dataloader(signals, labels, batch_size=batch_size, shuffle=False):\n",
    "    signals = torch.tensor(signals, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "    dataset = TensorDataset(signals, labels)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0)\n",
    "\n",
    "test_loader = create_dataloader(test_signals, test_labels, batch_size=batch_size, shuffle=False)\n",
    "print(\"Test DataLoader ready.\")\n",
    "\n",
    "#######################################################################################\n",
    "## Define the Attention Mechanism Module \n",
    "#######################################################################################\n",
    "    \n",
    "# Define the attention mechanism module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        # Linear layer to compute attention scores, keeping the input dimension\n",
    "        self.attention_weights = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        \n",
    "        # Bias term for the addition step (b)\n",
    "        self.bias = nn.Parameter(torch.zeros(input_dim))  # Bias term\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Dot product between input and weight matrix (e_t = x · W)\n",
    "        scores = self.attention_weights(x)  # Shape: (batch_size, input_dim)\n",
    "\n",
    "        # Apply hyperbolic tangent activation (h_t = tanh(e_t))\n",
    "        tanh_scores = torch.tanh(scores)\n",
    "\n",
    "        # Add bias to the result (y_t = h_t + b)\n",
    "        biased_scores = tanh_scores + self.bias\n",
    "\n",
    "        # Softmax to calculate attention weights (a_t = softmax(y_t))\n",
    "        attention_weights = torch.softmax(biased_scores, dim=-1)  # Shape: (batch_size, input_dim)\n",
    "\n",
    "        # Element-wise multiplication of the input with the attention weights\n",
    "        weighted_sum = x * attention_weights  # Element-wise multiplication\n",
    "\n",
    "        return weighted_sum\n",
    "\n",
    "#######################################################################################\n",
    "## Define the CardIA-Net model\n",
    "#######################################################################################\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x)\n",
    "\n",
    "class InceptionModulePlus(nn.Module):\n",
    "    def __init__(self, in_channels, num_kernels=32, bottleneck_channels=32):\n",
    "        super(InceptionModulePlus, self).__init__()\n",
    "        self.bottleneck = ConvBlock(in_channels, bottleneck_channels, kernel_size=1)\n",
    "        self.convs = nn.ModuleList([\n",
    "            ConvBlock(bottleneck_channels, num_kernels, kernel_size=39, padding=19),\n",
    "            ConvBlock(bottleneck_channels, num_kernels, kernel_size=19, padding=9),\n",
    "            ConvBlock(bottleneck_channels, num_kernels, kernel_size=9, padding=4)\n",
    "        ])\n",
    "        self.mp_conv = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(in_channels, bottleneck_channels, kernel_size=1)\n",
    "        )\n",
    "        self.concat = nn.Identity()\n",
    "        self.norm = nn.BatchNorm1d(num_kernels * 3 + bottleneck_channels)\n",
    "        self.act = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x_bottleneck = self.bottleneck(x)\n",
    "        conv_outputs = [conv(x_bottleneck) for conv in self.convs]\n",
    "        mp_output = self.mp_conv(x)\n",
    "        concatenated = torch.cat(conv_outputs + [mp_output], dim=1)\n",
    "        concatenated = self.norm(concatenated)\n",
    "        return self.act(concatenated)\n",
    "\n",
    "class InceptionBlockPlus(nn.Module):\n",
    "    def __init__(self, in_channels, blocks=num_incep_block, num_kernels=32, bottleneck_channels=32):\n",
    "        super(InceptionBlockPlus, self).__init__()\n",
    "        self.inception = nn.ModuleList([\n",
    "            InceptionModulePlus(in_channels if i == 0 else num_kernels * 3 + bottleneck_channels)\n",
    "            for i in range(blocks)\n",
    "        ])\n",
    "        self.shortcut = nn.ModuleList([\n",
    "            ConvBlock(in_channels, num_kernels * 3 + bottleneck_channels, kernel_size=1),\n",
    "            nn.BatchNorm1d(num_kernels * 3 + bottleneck_channels)\n",
    "        ])\n",
    "        self.act = nn.ModuleList([nn.ReLU(), nn.ReLU()])\n",
    "        self.add = torch.add\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        for module in self.shortcut:\n",
    "            residual = module(residual)\n",
    "        for module in self.inception:\n",
    "            x = module(x)\n",
    "        x = self.add(x, residual)\n",
    "        for activation in self.act:\n",
    "            x = activation(x)\n",
    "        return x\n",
    "\n",
    "class GAP1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAP1d, self).__init__()\n",
    "        self.gap = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "    def forward(self, x):\n",
    "        x = self.gap(x)\n",
    "        return self.flatten(x)\n",
    "\n",
    "class LinBnDrop(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias):\n",
    "        super(LinBnDrop, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias)\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class InceptionTimePlus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InceptionTimePlus, self).__init__()\n",
    "        self.backbone = nn.Sequential(InceptionBlockPlus(in_channels=CH_No))\n",
    "        self.head = nn.Sequential(GAP1d())\n",
    "        self.attention = Attention(input_dim=128)\n",
    "        self.final = nn.Sequential(LinBnDrop(in_features=128, out_features=3, bias=True))\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        x = self.attention(x)\n",
    "        return self.final(x)\n",
    "\n",
    "#######################################################################################\n",
    "# Build model, load trained parameters from working directory\n",
    "#######################################################################################\n",
    "model = InceptionTimePlus().to(device)\n",
    "print(model)\n",
    "\n",
    "# Load weights (no training)\n",
    "state_dict = torch.load(mod_weight, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "print(\"Loaded trained parameters from\", mod_weight)\n",
    "\n",
    "# Loss (for reporting test loss only; class weights are not required for testing)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#######################################################################################\n",
    "# Testing loop\n",
    "#######################################################################################\n",
    "model.eval()\n",
    "test_loss, correct, test_samples = 0.0, 0, 0\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        test_samples += labels.size(0)\n",
    "        all_preds.extend(preds.view(-1).cpu().numpy())\n",
    "        all_labels.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "print(\"Testing Done.\")\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = 100.0 * correct / test_samples\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Metrics (same as your code)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "report = classification_report(all_labels, all_preds)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0408e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
